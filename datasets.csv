name,tags,description,website,github,paper,hf_dataset_name,hf_contributor_handle
BasCrawl,modelado del lenguaje,BasCrawl es un corpus web de 186 millones de tokens en euskera obtenido mediante el análisis de más de 12000 dominios en internet (se incluyen los dominios analizados). El corpus ha sido preprocesado y depurado siguiendo el mismo procedimiento que MarIA.,https://doi.org/10.5281/zenodo.7313092,,,,
Biomedical Spanish CBOW Word Embeddings in Floret,"modelado del lenguaje,CBOW (Continuous Bag Of Words)","Dataset compuesto por embeddings entrenados en la combinación de todos los textos presentes en el corpus biomédico español, que incluye datos de múltiples fuentes para un total de 1100M tokens a través de 2,5M de documentos.",https://doi.org/10.5281/zenodo.7314041,https://arxiv.org/abs/2109.07765,,,
CSIC Spanish Corpus,modelado del lenguaje,El corpus español de CSIC es un corpus de 146 millones de tokens de revistas científicas españolas del repositorio revistas.csic.es/. El corpus se ha preprocesado y depurado mediante el procedimiento de Corpus-Cleaner.,https://doi.org/10.5281/zenodo.7313126,,,,
Catalonia Independence Corpus,clasificación de sentimientos,"Esta base de datos contiene dos corpus en español y catalán que contienen mensajes de Twitter anotados para la detección de opiniones. Cada corpus está anotado con tres posturas: 'against', 'favor' y 'neutral' (a favor, en contra, neutral) respecto a la independencia de Cataluña.",,https://github.com/ixa-ehu/catalonia-independence-corpus,https://www.aclweb.org/anthology/2020.lrec-1.171/,catalonia_independence,lewtun
HEAD-QA,preguntas de opción múltiple,HEAD-QA es un conjunto de datos de preguntas de opción múltiple sobre medicina. Las preguntas proceden de exámenes para acceder a un puesto en el sistema sanitario español y suponen un reto incluso para humanos altamente especializados.,https://aghie.github.io/head-qa/,https://github.com/aghie/head-qa,https://www.aclweb.org/anthology/P19-1092/,head_qa,mariagrandury
InfoLibros Corpus,modelado del lenguaje,El corpus InfoLibros es un corpus de 218 millones de tokens de narraciones en español extraídas de libros gratuitos recopilados por el proyecto abierto Infolibros.org. El corpus se ha preprocesado y depurado mediante el procedimiento de Corpus-Cleaner.,https://doi.org/10.5281/zenodo.7313105,,,,
Large Spanish Corpus,"modelado del lenguaje,pre-entrenamiento",El Large Spanish Corpus es una compilación de 15 corpus españoles sin etiquetar que abarcan desde la Wikipedia hasta las notas del Parlamento Europeo. Cada configuración contiene los datos correspondientes a cada corpus diferente.,,https://github.com/josecannete/spanish-corpora,,large_spanish_corpus,lewtun
Mucho Cine,clasificación de sentimientos,"El conjunto de datos de reseñas de Muchocine contiene 3.872 reseñas de películas en español, cada una de ellas con un breve resumen y una calificación en una escala de 1 a 5.",http://www.lsi.us.es/~fermin/index.php/Datasets,,,muchocine,mapmeld
Spanish Billion Words,"modelado del lenguaje,pre-entrenamiento","Spanish Billion Words es un corpus no anotado de casi 1.500 millones de palabras, compuesto por diferentes recursos online.",https://crscardellino.github.io/SBWCE/,,,spanish_billion_words,mariagrandury
Spanish Biomedical Crawled Corpus,modelado del lenguaje,"El mayor corpus biomédico y de salud en español hasta la fecha, recopilado a partir de un análisis web masivo de dominios de salud españoles en más de 3.000 URL. Todos los datos recopilados se han preprocesado para producir el recurso CoWeSe (Corpus Web Salud Español).",https://doi.org/10.5281/zenodo.5513237,,https://arxiv.org/abs/2109.07765,,
Spanish CBOW Word Embeddings in FastText,"modelado del lenguaje,FastText","Embeddings de palabras en español en FastText generados a partir del mayor corpus realizado en español hasta la fecha. El corpus cuenta con más de 2 TB de texto de alta calidad, recopilado a partir de los diferentes rastreos web realizados por la Biblioteca Nacional de España entre 2009 y 2019. Dataset compuesto exclusivamente por CBOW embeddings.",https://doi.org/10.5281/zenodo.5044988,,,http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6405,
Spanish CBOW Word Embeddings in Floret,"modelado del lenguaje,CBOW (Continuous Bag Of Words)",Dataset compuesto por embeddings entrenados con el corpus de la Biblioteca Nacional de España (BNE) utilizando floret.,https://doi.org/10.5281/zenodo.7314098,,,,
Spanish Legal Domain Corpora,modelado del lenguaje,Dataset compuesto por una colección de textos (corpus) del ámbito jurídico español.,https://doi.org/10.5281/zenodo.5495529,https://github.com/PlanTL-GOB-ES/lm-legal-es,https://arxiv.org/abs/2110.12201,,
Spanish Legal Domain Word & Sub-Word Embeddings,modelado del lenguaje,Conjunto de embeddings generados a partir del corpus compuesto de recursos jurídicos españoles más grande hasta la fecha (9GB).,https://doi.org/10.5281/zenodo.5036147,https://github.com/PlanTL-GOB-ES/lm-legal-es,https://arxiv.org/abs/2110.12201,,
Spanish Skip-Gram Word Embeddings in FastText,"modelado del lenguaje,FastText","El corpus cuenta con más de 2TB de texto de alta calidad, recopilado a partir de los diferentes análisis web realizados por la Biblioteca Nacional de España desde 2009 hasta 2019. Dataset compuesto exclusivamente por embeddings Skip-Gram.",https://doi.org/10.5281/zenodo.5046525,,,http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6405,
TDX Thesis Spanish Corpus,modelado del lenguaje,"El corpus TDX Thesis Spanish es un corpus de 246 millones de tokens de texto limpio en español extraído de tesis científicas del dominio tdx.cat, que contiene tesis abiertas publicadas por universidades catalanas. El corpus se ha preprocesado y depurado mediante el procedimiento de Corpus-Cleaner.",https://doi.org/10.5281/zenodo.7313149,,,,
WikiCorpus,"modelado del lenguaje,POS (Part of Speech)","El Wikicorpus es un corpus trilingüe (catalán, español, inglés) que contiene grandes partes de la Wikipedia de 2006 y que ha sido enriquecido automáticamente con información lingüística. En su versión actual, contiene más de 750 millones de palabras.",https://www.cs.upc.edu/~nlp/wikicorpus/,,https://www.cs.upc.edu/~nlp/papers/reese10.pdf,wikicorpus,albertvillanova
eHealth-KD,NER (Named Entity Recognition),Base de datos del challenge eHealth-KD de IberLEF 2020. Está diseñado para la identificación de entidades y relaciones semánticas en documentos sanitarios españoles.,https://knowledge-learning.github.io/ehealthkd-2020/,https://github.com/knowledge-learning/ehealthkd-2020,http://ceur-ws.org/Vol-2664/eHealth-KD_overview.pdf,ehealth_kd,mariagrandury
